import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.utils as vutils
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
from cleanfid import fid
from pytorch_fid import fid_score


# ----------------------------
# Hyperparameters
# ----------------------------
BATCH_SIZE = 256
IMAGE_SIZE = 32
NC = 3  # Number of channels
NZ = 100  # Size of latent vector
NGF = 64  # Generator feature maps
NDF = 64  # Discriminator feature maps
EPOCHS = 1
LR = 0.0002
BETA1 = 0.5  # Beta1 hyperparam for Adam optimizers

# ----------------------------
# Data Loading and Preprocessing
# ----------------------------
def save_cifar10_images_for_fid(folder="cifar10_dataset"):
    """Save a subset of CIFAR-10 test images for FID evaluation"""
    transform = transforms.Compose([transforms.ToTensor()])
    
    # Load test dataset only (for FID evaluation)
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    
    os.makedirs(f"{folder}/test", exist_ok=True)
    
    # Save only test images (no class folders needed for FID)
    print("Saving test images for FID evaluation...")
    for i, (img, label) in enumerate(test_dataset):
        if i >= 5000:  # Only save first 5000 images for faster FID computation
            break
        img_pil = transforms.ToPILImage()(img)
        img_pil.save(f"{folder}/test/img_{i}.png")
    
    print(f"Saved {min(len(test_dataset), 5000)} test images for FID evaluation")

# ----------------------------
# Generator Model
# ----------------------------
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            # Input is Z, going into a convolution
            nn.ConvTranspose2d(NZ, NGF * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(NGF * 8),
            nn.ReLU(True),
            # state size: (ngf*8) x 4 x 4
            nn.ConvTranspose2d(NGF * 8, NGF * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(NGF * 4),
            nn.ReLU(True),
            # state size: (ngf*4) x 8 x 8
            nn.ConvTranspose2d(NGF * 4, NGF * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(NGF * 2),
            nn.ReLU(True),
            # state size: (ngf*2) x 16 x 16
            nn.ConvTranspose2d(NGF * 2, NC, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size: (nc) x 32 x 32
        )

    def forward(self, input):
        return self.main(input)

# ----------------------------
# Discriminator Model
# ----------------------------
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # Input is (nc) x 32 x 32
            nn.Conv2d(NC, NDF, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size: (ndf) x 16 x 16
            nn.Conv2d(NDF, NDF * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(NDF * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size: (ndf*2) x 8 x 8
            nn.Conv2d(NDF * 2, NDF * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(NDF * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size: (ndf*4) x 4 x 4
            nn.Conv2d(NDF * 4, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
            # state size: 1 x 1 x 1
        )

    def forward(self, input):
        return self.main(input).view(-1, 1).squeeze(1)

# ----------------------------
# Weight Initialization
# ----------------------------
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# ----------------------------
# Training Loop
# ----------------------------
def train_gan(generator, discriminator, train_loader, device):
    # Lists to keep track of progress
    img_list = []
    G_losses = []
    D_losses = []
    
    # Loss Function and Optimizers
    criterion = nn.BCELoss()
    
    # Use equal learning rates for both optimizers
    optimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(BETA1, 0.999))
    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(BETA1, 0.999))
    
    # Fixed noise for visualization
    fixed_noise = torch.randn(64, NZ, 1, 1, device=device)
    
    print("Starting Training Loop...")
    
    for epoch in range(EPOCHS):
        epoch_d_loss = 0.0
        epoch_g_loss = 0.0
        num_batches = 0
        
        for i, (data, _) in enumerate(train_loader):
            ############################
            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
            ###########################
            discriminator.zero_grad()
            real_data = data.to(device)
            batch_size = real_data.size(0)
            
            # Train with real data (label smoothing)
            label_real = torch.full((batch_size,), 0.95, dtype=torch.float, device=device)
            output_real = discriminator(real_data)
            loss_D_real = criterion(output_real, label_real)
            loss_D_real.backward()
            
            # Train with fake data (label smoothing)
            noise = torch.randn(batch_size, NZ, 1, 1, device=device)
            fake_data = generator(noise)
            label_fake = torch.full((batch_size,), 0.05, dtype=torch.float, device=device)
            output_fake = discriminator(fake_data.detach())
            loss_D_fake = criterion(output_fake, label_fake)
            loss_D_fake.backward()
            
            loss_D = loss_D_real + loss_D_fake
            optimizer_D.step()
            
            ############################
            # (2) Update G network: maximize log(D(G(z)))
            ###########################
            generator.zero_grad()
            # Generate fresh fake data for generator training
            noise = torch.randn(batch_size, NZ, 1, 1, device=device)
            fake_data_for_G = generator(noise)
            label_real_for_G = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)
            output_fake_for_G = discriminator(fake_data_for_G)
            loss_G = criterion(output_fake_for_G, label_real_for_G)
            loss_G.backward()
            optimizer_G.step()
            
            # Statistics
            epoch_d_loss += loss_D.item()
            epoch_g_loss += loss_G.item()
            num_batches += 1
            
            # Print statistics less frequently
            if i % 100 == 0:
                print(f'[{epoch+1}/{EPOCHS}][{i}/{len(train_loader)}] '
                      f'Loss_D: {loss_D.item():.4f} Loss_G: {loss_G.item():.4f}')
        
        # Calculate average losses for the epoch
        avg_d_loss = epoch_d_loss / num_batches
        avg_g_loss = epoch_g_loss / num_batches
        
        G_losses.append(avg_g_loss)
        D_losses.append(avg_d_loss)
        
        print(f'Epoch [{epoch+1}/{EPOCHS}] completed - Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}')
        
        # Save sample images every 10 epochs
        if (epoch + 1) % 10 == 0:
            with torch.no_grad():
                fake_sample = generator(fixed_noise)
                img_list.append(vutils.make_grid(fake_sample, padding=2, normalize=True))
    
    return G_losses, D_losses, img_list


# ----------------------------
# Generate and Save Images for FID
# ----------------------------
def save_generated_images(generator, device, out_dir="generated_images", num_images=5000):
    os.makedirs(out_dir, exist_ok=True)
    
    generator.eval()
    generated_count = 0
    
    print(f"Generating {num_images} images for FID evaluation...")
    with torch.no_grad():
        while generated_count < num_images:
            # Generate batch
            batch_size = min(100, num_images - generated_count)
            noise = torch.randn(batch_size, NZ, 1, 1, device=device)
            fake_images = generator(noise)
            
            # Convert to numpy and denormalize
            fake_images = fake_images.cpu()
            fake_images = (fake_images + 1) / 2.0  # Denormalize from [-1,1] to [0,1]
            fake_images = torch.clamp(fake_images, 0, 1)
            
            # Save images
            for i in range(batch_size):
                img_tensor = fake_images[i]
                img_pil = transforms.ToPILImage()(img_tensor)
                img_pil.save(f"{out_dir}/img_{generated_count}.png")
                generated_count += 1
            
            # Print progress less frequently
            if generated_count % 1000 == 0 or generated_count == num_images:
                print(f"Generated {generated_count}/{num_images} images")
    
    generator.train()

# ----------------------------
# Visualization Function
# ----------------------------
def plot_losses(G_losses, D_losses):
    plt.figure(figsize=(10, 5))
    plt.title("Generator and Discriminator Loss During Training")
    plt.plot(G_losses, label="Generator Loss")
    plt.plot(D_losses, label="Discriminator Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.savefig("training_losses.png")
    plt.show()

# ----------------------------
# Main Execution
# ----------------------------
if __name__ == "__main__":
    # ----------------------------
    # GPU Configuration
    # ----------------------------
    print("PyTorch version:", torch.__version__)
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        # Clear GPU cache
        torch.cuda.empty_cache()

    print("Training DCGAN on PyTorch...")
    print(f"Device: {device}")
    print(f"Batch size: {BATCH_SIZE}")
    print(f"Epochs: {EPOCHS}")
    
    # Save test images for FID if they don't exist
    if not os.path.exists("cifar10_dataset/test"):
        print("Preparing CIFAR-10 test images for FID evaluation...")
        save_cifar10_images_for_fid()
        print("Test images ready for FID evaluation")

    # Create dataset and dataloader
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
    ])

    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)

    # ----------------------------
    # Create Models
    # ----------------------------
    generator = Generator().to(device)
    discriminator = Discriminator().to(device)

    # Apply weight initialization
    generator.apply(weights_init)
    discriminator.apply(weights_init)

   
    # Train the GAN
    G_losses, D_losses, img_list = train_gan(generator, discriminator, train_loader, device)
    
    # Plot losses
    plot_losses(G_losses, D_losses)
    
    # Generate images for FID evaluation
    print("Generating evaluation images...")
    save_generated_images(generator, device, num_images=5000)
    
    # Compute FID score\
    print("Calculating FID score...")
    fid_value = fid_score.calculate_fid_given_paths(
    ["generated_images", "cifar10_dataset/test"],
    batch_size=50,
    device='cuda' if torch.cuda.is_available() else 'cpu',
    dims=2048
    )
    print(f"FID Score: {fid_value:.2f}")
        
    # Save the trained models
    torch.save(generator.state_dict(), 'generator.pth')
    torch.save(discriminator.state_dict(), 'discriminator.pth')
    print("Models saved as generator.pth and discriminator.pth")
    
    # Display some generated images
    if img_list:
        plt.figure(figsize=(15, 15))
        plt.subplot(1, 1, 1)
        plt.axis("off")
        plt.title("Generated Images (Final)")
        plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))
        plt.savefig("final_generated_images.png")
        plt.show()
